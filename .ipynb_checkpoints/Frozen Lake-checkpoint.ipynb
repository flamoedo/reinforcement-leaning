{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "First we’re importing all the libraries we’ll be using. Not many, really... Numpy, gym, random, \n",
    "time, and clear_output from Ipython’s display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the environment\n",
    "Next, to create our environment, we just call gym.make() and pass a string of the name of the environment we want to set up. We'll be using the environment FrozenLake-v0. All the environments with their corresponding names you can use here are available on Gym’s website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this env object, we’re able to query for information about the environment, sample states and actions, retrieve rewards, and have our agent navigate the frozen lake. That’s all made available to us conveniently with Gym.\n",
    "\n",
    "### Creating the Q-table\n",
    "We’re now going to construct our Q-table, and initialize all the Q-values to zero for each state-action pair.\n",
    "\n",
    "Remember, the number of rows in the table is equivalent to the size of the state space in the environment, and the number of columns is equivalent to the size of the action space. We can get this information using using env.observation_space.n and env.action_space.n, as shown below. We can then use this information to build the Q-table and fill it with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n\n",
    "\n",
    "q_table = np.zeros((state_space_size, action_space_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're foggy about Q-tables at all, be sure to check out the earlier post where we covered all the details you need for Q-tables.\n",
    "\n",
    "Alright, here’s our Q-table!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Q-learning parameters\n",
    "Now, we’re going to create and initialize all the parameters needed to implement the Q-learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10000\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding the Q-learning algorithm training loop\n",
    "Let’s start from the top.\n",
    "\n",
    "First, we create this list to hold all of the rewards we’ll get from each episode. This will be so we can see how our game score changes over time. We’ll discuss this more in a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following block of code, we’ll implement the entire Q-learning algorithm we discussed in detail in a couple posts back. When this code is executed, this is exactly where the training will take place. This first for-loop contains everything that happens within a single episode. This second nested loop contains everything that happens for a single time-step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each episode\n",
    "Let's get inside of our first loop. For each episode, we’re going to first reset the state of the environment back to the starting state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward per thousand episodes********\n",
      "\n",
      "1000 :  0.1330000000000001\n",
      "2000 :  0.19400000000000014\n",
      "3000 :  0.43500000000000033\n",
      "4000 :  0.6520000000000005\n",
      "5000 :  0.6730000000000005\n",
      "6000 :  0.6570000000000005\n",
      "7000 :  0.6570000000000005\n",
      "8000 :  0.6540000000000005\n",
      "9000 :  0.6580000000000005\n",
      "10000 :  0.6470000000000005\n"
     ]
    }
   ],
   "source": [
    "rewards_all_episodes = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "\n",
    "    for step in range(max_steps_per_episode): \n",
    "        # Exploration-exploitation trade-off\n",
    "        exploration_rate_threshold = random.uniform(0, 1)\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:]) \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "    \n",
    "        # Update Q-table for Q(s,a)\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
    "            learning_rate * (reward + discount_rate * np.max(q_table[new_state, : ]))\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "        rewards_current_episode += reward\n",
    "\n",
    "        if done == True: \n",
    "            break\n",
    "                         \n",
    "    # Exploration rate decay\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "        \n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "                         \n",
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thosand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"********Average reward per thousand episodes********\\n\")\n",
    "for r in rewards_per_thosand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********Q-table********\n",
      "\n",
      "[[0.53246314 0.39512583 0.45358368 0.42912783]\n",
      " [0.31566212 0.08388292 0.02626351 0.00641208]\n",
      " [0.24667491 0.14414996 0.08974041 0.07320063]\n",
      " [0.09373453 0.         0.         0.        ]\n",
      " [0.5585331  0.441068   0.406648   0.35329727]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.26599133 0.08184318 0.12524373 0.02233389]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.23201432 0.34788052 0.48588485 0.58493298]\n",
      " [0.35148606 0.64310965 0.40214019 0.41055741]\n",
      " [0.59275477 0.35678002 0.34079595 0.2608085 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.38724894 0.47902905 0.73146844 0.46171325]\n",
      " [0.65396024 0.8717434  0.74349489 0.6971267 ]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Print updated Q-table\n",
    "print(\"\\n\\n********Q-table********\\n\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The done variable just keeps track of whether or not our episode is finished, so we initialize it to False when we first start the episode, and we'll see later where it will get updated to notify us when the episode is over.\n",
    "\n",
    "Then, we need to keep track of the rewards within the current episode as well, so we set rewards_current_episode to 0 since we start out with no rewards at the beginning of each episode.\n",
    "\n",
    "### For each time-step\n",
    "Now we're entering into the nested loop, which runs for each time-step within an episode. The remaining steps, until we say otherwise, will occur for each time-step.\n",
    "\n",
    "### Exploration vs. exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each time-step within an episode, we set our exploration_rate_threshold to a random number between 0 and 1. This will be used to determine whether our agent will explore or exploit the environment in this time-step, and we discussed the detail of this exploration-exploitation trade-off in a previous post of this series.\n",
    "\n",
    "If the threshold is greater than the exploration_rate, which remember, is initially set to 1, then our agent will exploit the environment and choose the action that has the highest Q-value in the Q-table for the current state. If, on the other hand, the threshold is less than or equal to the exploration_rate, then the agent will explore the environment, and sample an action randomly.\n",
    "\n",
    "### Taking action"
   ]
  },
  {
   "attachments": {
    "Screenshot_2.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After our action is chosen, we then take that action by calling step() on our env object and passing our action to it. The function step() returns a tuple containing the new state, the reward for the action we took, whether or not the action ended our episode, and diagnostic information regarding our environment, which may be helpful for us if we end up needing to do any debugging.\n",
    "\n",
    "Update the Q-value\n",
    "After we observe the reward we obtained from taking the action from the previous state, we can then update the Q-value for that state-action pair in the Q-table. This is done using the formula we introduced in an earlier post, and remember, there we walked through a concrete example showing how to implement the Q-table update.\n",
    "\n",
    "Here is the forumla:\n",
    "\n",
    "![Screenshot_2.png](attachment:Screenshot_2.png)\n",
    "\n",
    "And here is the same formula in code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, remember, the new Q-value for this state-action pair is a weighted sum of our old value and the “learned value.” So we have our new Q-value equal to the old Q-value times one minus the learning rate plus the learning rate times the “learned value,” which is the reward we just received from our last action plus the discounted estimate of the optimal future Q-value for the next state action pair.\n",
    "\n",
    "### Transition to the next state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set our current state to the new_state that was returned to us once we took our last action, and we then update the rewards from our current episode by adding the reward we received for our previous action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then check to see if our last action ended the episode for us, meaning, did our agent step in a hole or reach the goal? If the action did end the episode, then we jump out of this loop and move on to the next episode. Otherwise, we transition to the next time-step.\n",
    "\n",
    "### Exploration rate decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once an episode is finished, we need to update our exploration_rate using exponential decay, which just means that the exploration rate decreases or decays at a rate proportional to its current value. We can decay the exploration_rate using the formula above, which makes use of all the exploration rate parameters that we defined last time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then just append the rewards from the current episode to the list of rewards from all episodes, and that’s it! We’re good to move on to the next episode.\n",
    "\n",
    "### After all episodes complete\n",
    "After all episodes are finished, we now just calculate the average reward per thousand episodes from our list that contains the rewards for all episodes so that we can print it out and see how the rewards changed over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this printout, we can see our average reward per thousand episodes did indeed progress over time. When the algorithm first started training, the first thousand episodes only averaged a reward of 0.16, but by the time it got to its last thousand episodes, the reward drastically improved to 0.7.\n",
    "\n",
    "Interpreting the training results\n",
    "Let’s take a second to understand how we can interpret these results. Our agent played 10,000 episodes. At each time step within an episode, the agent received a reward of 1 if it reached the frisbee, otherwise, it received a reward of 0. If the agent did indeed reach the frisbee, then the episode finished at that time-step.\n",
    "\n",
    "So, that means for each episode, the total reward received by the agent for the entire episode is either 1 or 0. So, for the first thousand episodes, we can interpret this score as meaning that \n",
    "16\n",
    "%\n",
    " of the time, the agent received a reward of 1 and won the episode. And by the last thousand episodes from a total of 10,000, the agent was winning \n",
    "70\n",
    "%\n",
    " of the time.\n",
    "\n",
    "By analyzing the grid of the game, we can see it’s a lot more likely that the agent would fall in a hole or perhaps reach the max time steps than it is to reach the frisbee, so reaching the frisbee \n",
    "70\n",
    "%\n",
    " of the time isn’t too shabby, especially since the agent had no explicit instructions to reach the frisbee. It learned that this is the correct thing to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFFF\n",
    "# FHFH\n",
    "# FFFH\n",
    "# HFFG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we print out our updated Q-table to see how that has transitioned from its initial state of all zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapping up\n",
    "In the next post, we’re going to get into the super fun part where we get to watch our trained agent play Frozen Lake. We’ll get straight into the code for that then!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch our agent play Frozen Lake by playing the best action \n",
    "# from each state according to the Q-table\n",
    "\n",
    "for episode in range(3):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "\n",
    "    for step in range(max_steps_per_episode):        \n",
    "        # Show current state of environment on screen\n",
    "        # Choose action with highest Q-value for current state       \n",
    "        # Take new action\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        action = np.argmax(q_table[state,:])        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print(\"****You reached the goal!****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"****You fell through a hole!****\")\n",
    "                time.sleep(3)\n",
    "                clear_output(wait=True)\n",
    "            break\n",
    "            \n",
    "        state = new_state\n",
    "\n",
    "        \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
